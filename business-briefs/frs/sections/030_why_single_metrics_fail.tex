\section*{Why Single Metrics Fail to Capture Readiness}

Single metrics simplify evaluation, but they also conceal risk. Accuracy metrics average away instability, reliability measures ignore severity, and cost-based metrics depend on assumptions that may not hold uniformly. Each captures part of the picture, but none can stand alone.

Optimizing any one metric can produce perverse outcomes. A forecast may reduce average error by shifting risk into rare but catastrophic failures. Another may minimize cost under assumed weights while becoming unreliable when conditions change. These trade-offs are invisible when metrics are considered in isolation.

Because operational failure is multidimensional, readiness cannot be inferred from any single signal. A forecast that scores well on one measure may still be unfit for deployment if it fails on another. Treating readiness as a single-number optimization problem invites fragile solutions and false confidence.

The failure of single metrics is not a flaw in their design. It is a mismatch between what they measure and what readiness requires. Readiness is a threshold judgment across dimensions, not a continuous improvement along one axis.
