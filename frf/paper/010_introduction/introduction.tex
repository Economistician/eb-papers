\section{Introduction}

Short-horizon forecasts play a central role in operational decision-making across a wide range of
settings, including production planning, staffing, inventory replenishment, logistics, and service
operations. In these environments, forecasts are not evaluated in isolation; they are used to
determine whether sufficient capacity, labor, or product will be available at specific moments in
time. As a result, forecast errors translate directly into operational outcomes such as unmet demand,
service delays, recovery time, and resource inefficiency. Forecast quality in these contexts is therefore
best understood not only in terms of numerical accuracy, but in terms of whether forecasts are
\emph{ready} to support reliable execution under real operational constraints.

Despite this operational reality, forecast performance is most commonly assessed using symmetric
accuracy metrics such as mean absolute error (MAE), root mean squared error (RMSE), and mean
absolute percentage error (MAPE). These measures implicitly assume that over-forecasting and
under-forecasting impose equivalent cost. In many high-frequency operational systems, however,
this assumption does not hold. Shortages frequently lead to service failures, lost throughput, or
cascading recovery effects, whereas excess capacity or production is often absorbed at comparatively
low cost. As a consequence, forecasts that perform well according to symmetric metrics may still
exhibit systematic failure patterns during critical intervals, particularly when errors are concentrated
in the direction of under-forecasting.

A substantial literature in operations management and forecasting recognizes the asymmetric cost
of shortages and excess. Classic models such as the newsvendor formulation explicitly distinguish
between underage and overage cost, and asymmetric loss functions are widely used in model training
and optimization. However, these approaches are primarily concerned with decision optimization
rather than forecast evaluation. Moreover, they typically reduce performance to a single scalar loss,
providing limited visibility into how error frequency, severity, tolerance, and economic exposure
jointly affect deployment readiness. As a result, existing methods offer limited guidance for
diagnosing why a forecast that appears accurate by aggregate measures fails to support reliable
execution once embedded in an operational system.

In this paper, we argue that evaluating forecasts in asymmetric operational environments requires a
multi-dimensional perspective centered on readiness for deployment rather than numerical accuracy
alone. We introduce the \emph{Forecast Readiness Framework} (FRF), a structured and governed
evaluation framework that decomposes readiness into complementary dimensions capturing service
reliability, failure severity, tolerance stability, and economic consequence. Within this framework,
Cost-Weighted Service Loss (CWSL) serves as the primary economic axis, explicitly quantifying the
asymmetric operational impact of forecast error. Supporting diagnostic measures characterize how
frequently shortfalls occur, how severe they are when they arise, and whether deviations remain
within bounds that can be absorbed by the operational system under stated assumptions.

Crucially, FRF treats cost asymmetry, tolerance thresholds, and admissibility rules as \emph{governed
evaluation assumptions} rather than free modeling parameters. Sensitivity analysis, calibration
procedures, and policy-based constraints are used to make these assumptions explicit, inspectable,
and auditable, ensuring that readiness assessments remain interpretable and stable across models,
entities, and time periods.

The contributions of this paper are threefold. First, we formalize forecast readiness as a distinct
evaluation objective that cannot be captured by symmetric accuracy metrics alone. Second, we
propose a governed, multi-dimensional framework that integrates reliability, severity, tolerance, and
cost diagnostics to assess whether forecasts are fit for operational deployment in asymmetric
environments. Third, we demonstrate how readiness-oriented evaluation reveals failure modes and
risk exposures that are systematically obscured by traditional performance measures, and how these
diagnostics support transparent deployment decisions and ongoing forecast governance. The
remainder of the paper reviews related work, introduces the Forecast Readiness Framework in detail,
presents an illustrative example, and discusses implications for evaluation practice, governance, and
future research.
