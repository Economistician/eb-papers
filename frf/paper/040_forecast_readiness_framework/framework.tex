\section{The Forecast Readiness Framework}

In asymmetric operational environments, forecast evaluation cannot be reduced to a single notion
of accuracy. Forecasts are used to support execution under constraints, uncertainty, and time
pressure, and different patterns of error impose qualitatively different operational consequences.
A forecast that occasionally produces deep shortages poses a different type of risk than one that
is consistently biased but stable, even if both achieve similar aggregate accuracy. Evaluating such
forecasts therefore requires a framework that distinguishes among the dimensions of error that
matter for operational readiness.

The \emph{Forecast Readiness Framework} (FRF) formalizes this perspective by decomposing
readiness into a small set of complementary diagnostic dimensions, together with governed
assumptions and decision rules that determine how those diagnostics are interpreted in practice.
Each dimension captures a distinct aspect of how forecast error affects execution, and no single
dimension is sufficient on its own. Together, these components provide a structured and auditable
basis for assessing whether a forecast is fit for deployment in environments where under-forecasting
and over-forecasting have asymmetric cost.

\subsection{Readiness as a Multi-Dimensional Construct}

Forecast readiness is defined here as the degree to which a forecast reliably supports operational
execution under asymmetric error cost and stated tolerance assumptions. This definition emphasizes
deployment suitability rather than numerical precision. A forecast may achieve low average error
while still failing to meet readiness requirements if its errors are concentrated in the wrong
direction, occur too frequently, exceed the systemâ€™s ability to absorb deviations, or expose the
operation to unacceptable economic risk.

Within FRF, readiness is decomposed into four primary diagnostic dimensions:
\begin{enumerate}
    \item \textbf{Service reliability}: how often the forecast avoids shortfalls entirely.
    \item \textbf{Failure severity}: how large shortfalls are when they occur.
    \item \textbf{Tolerance stability}: how frequently deviations fall within operationally
    acceptable bounds.
    \item \textbf{Economic consequence}: the cost-weighted impact of directional forecast error.
\end{enumerate}

These dimensions are evaluated under explicitly governed assumptions, including cost asymmetry,
tolerance thresholds, and admissibility rules, which are treated as part of the evaluation contract
rather than free modeling parameters.

Figure~\ref{fig:frf_overview} summarizes the structure of the \FRF\ and the relationship between
its diagnostic dimensions, the economic axis of evaluation, and the composite readiness signal
used to support deployment, monitoring, and governance decisions.

\input{figures/frf_overview}

Each dimension captures information that is obscured when performance is summarized by a
single symmetric metric. For example, two forecasts with similar average error may differ sharply
in the frequency of shortfalls, the depth of those shortfalls, or the economic burden they impose
during peak intervals. FRF is designed to make these distinctions explicit and actionable.

\subsection{Diagnostic Components of the Framework}

FRF operationalizes the four readiness dimensions using a set of interpretable metrics. Service
reliability is measured by the \emph{No--Shortfall Level} (NSL), which quantifies the proportion of
intervals in which forecasted demand meets or exceeds realized demand. NSL isolates the
frequency of service-risk events without regard to their magnitude.

Failure severity is captured by \emph{Underbuild Depth} (UD), which measures the average
magnitude of shortfalls conditional on a shortfall occurring. UD distinguishes between forecasts
that miss frequently but shallowly and those that occasionally miss by large amounts, a distinction
with important implications for recovery dynamics and throughput loss.

Tolerance stability is measured by the \emph{Hit Rate within Tolerance} (\HRtau), which
quantifies the proportion of intervals in which forecast error falls within an operationally acceptable
bound. This metric reflects the fact that many operational systems can absorb small deviations
without degradation, and that readiness depends not on exact accuracy but on remaining within
decision-relevant tolerances.

Economic consequence is captured by \emph{Cost-Weighted Service Loss} (CWSL), which aggregates
directional forecast error using asymmetric penalties for shortfalls and overbuilds and normalizes
the result by realized demand. CWSL provides a cost-aligned measure of how forecast error
translates into effective operational loss under a specified penalty structure.

Each diagnostic admits both a raw form and a governed form, in which cost asymmetry and
tolerance assumptions are selected through sensitivity analysis and calibration procedures rather
than fixed heuristics. None of these diagnostics is intended to stand alone; each addresses a
distinct failure mode that symmetric accuracy measures systematically obscure.

\subsection{CWSL as the Economic Axis of Readiness}

Among the diagnostic components, CWSL plays a central role by providing an explicit economic
lens on forecast performance. Whereas NSL, UD, and \HRtau describe the structure and frequency
of error, CWSL translates that structure into cost-weighted consequence. This makes CWSL the
primary axis along which asymmetric operational risk is expressed within the framework.

Crucially, the cost asymmetry underlying CWSL is treated as a governed evaluation assumption.
Sensitivity analysis and calibration procedures are used to inspect how readiness assessments vary
across plausible cost ratios and to select reference values that balance historical underbuild and
overbuild exposure. This prevents evaluation outcomes from being driven by implicit or unstable
cost assumptions and supports transparent comparison across models and entities.

\subsection{Framework Interpretation and Use}

The Forecast Readiness Framework is intended to support evaluation, comparison, and governance
of forecasting systems rather than optimization alone. By examining multiple dimensions of
readiness under explicit assumptions, practitioners can identify why a forecast fails to support
execution, distinguish among competing failure modes, and select models whose error structure
aligns with operational priorities.

At an aggregate level, the framework supports comparison across models, items, locations, or
time periods using a consistent and auditable evaluation contract. At a diagnostic level, it
highlights whether poor performance arises from frequent shortfalls, severe shortfalls, instability
relative to tolerance, or misalignment between error structure and economic asymmetry. These
diagnostics may be synthesized into a composite readiness signal and, where appropriate, passed
through adjustment layers that enforce feasibility constraints without altering the underlying
forecast.

By reframing forecast evaluation around governed readiness rather than numerical accuracy alone,
the Forecast Readiness Framework provides a principled and operationally grounded basis for
assessing forecast performance in asymmetric environments.
