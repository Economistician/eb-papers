% ----------------------------------------------------------
% RELATED WORK
% ----------------------------------------------------------
\section{Related Work}

Forecast accuracy is traditionally assessed using symmetric error metrics such as mean
absolute error (MAE), root mean squared error (RMSE), and mean absolute percentage error
(MAPE). These metrics are widely adopted due to their simplicity, interpretability, and
compatibility with diverse modeling approaches \cite{hyndman2018}. However, they treat
forecast deviations symmetrically and thus implicitly assume that over-forecasting and
under-forecasting impose equivalent cost. In settings where shortages are substantially
more disruptive than excess, this assumption leads to evaluations that obscure the
operational consequences most relevant to managers \cite{goodwin2011}.

A long-standing literature in operations management emphasizes the asymmetric cost of
shortages and excess. The classic newsvendor formulation \cite{arrow1951} captures
underage and overage cost asymmetry and shows that optimal inventory decisions depend on
their relative magnitudes. Subsequent research extends these ideas to service-level
constraints, multi-echelon systems, perishability, and risk preferences. However, these
models are designed for \textit{decision optimization} rather than \textit{forecast
evaluation}, and typically do not characterize interval-level forecast error or provide
metrics for assessing forecast quality.

Asymmetric loss functions also appear in statistical and machine learning contexts.
Quantile (pinball) loss \cite{koenker2005} penalizes over-prediction and
under-prediction unequally, and is widely used for distributional forecasting and
predictive uncertainty estimation. Other work has explored tilted or cost-weighted loss
functions in classification and regression to encode domain-specific risk preferences
\cite{gneiting2011}. While related in spirit, these methods influence \textit{model
training} rather than evaluation of realized forecast output, and they do not normalize
costs by demand or incorporate operational readiness concepts.

Forecast evaluation specifically in operational environments has received increased
attention across domains such as retail replenishment \cite{ferreira2016}, call center
management \cite{audi2019}, workforce scheduling \cite{gans2003}, and intermittent
demand forecasting \cite{snyder2012}. These studies highlight the importance of aligning
forecasts with service levels, staffing requirements, or production constraints.
Nevertheless, existing evaluation approaches are primarily symmetric or rely on
volume-normalized measures such as wMAPE, which cannot differentiate between error
patterns that are operationally benign and those that produce readiness failures.

Across these literatures, we are not aware of an evaluation framework that jointly
integrates:
(1) explicit asymmetric penalties for shortfalls and overbuilds;
(2) demand normalization across items and intervals; and
(3) interval-level granularity suitable for high-frequency operational environments.

The Cost-Weighted Service Loss (CWSL) metric introduced in this paper fills this gap by
adapting asymmetric cost principles into a forecast evaluation framework that is both
operationally interpretable and widely applicable across domains where readiness and
service reliability are central performance objectives.